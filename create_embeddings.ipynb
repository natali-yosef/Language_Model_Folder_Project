{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "INSTALL STUFF"
      ],
      "metadata": {
        "id": "k-DTDQt-8gIP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KgMuCqMnoev"
      },
      "outputs": [],
      "source": [
        "!pip install biopython\n",
        "!pip install torch\n",
        "!pip install tape_proteins\n",
        "!pip install Bio\n",
        "!pip install import-ipynb\n",
        "!pip install git+https://github.com/facebookresearch/esm.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORT STUFF"
      ],
      "metadata": {
        "id": "-TL5qH2xkR3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import torch\n",
        "from Bio.PDB import PDBParser, Polypeptide, is_aa\n",
        "from esm import FastaBatchedDataset, pretrained\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "PwMCvjDwkUBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATE EMBEDDINGS FOR PDB FILES AND STORE THEM"
      ],
      "metadata": {
        "id": "G6E14jtG8Xi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_seq_aa(pdb_file, chain_id='A'):\n",
        "    chain = PDBParser(QUIET=True).get_structure(str(pdb_file.stem), str(pdb_file))[0][chain_id]\n",
        "    aa_residues = []\n",
        "    seq = \"\"\n",
        "    for residue in chain.get_residues():\n",
        "        aa = residue.get_resname()\n",
        "        if not is_aa(aa) or not residue.has_id('CA'): # Not amino acid\n",
        "            continue\n",
        "        elif aa == \"UNK\":  # unknown amino acid\n",
        "            seq += \"X\"\n",
        "        else:\n",
        "            seq += Polypeptide.three_to_one(residue.get_resname())\n",
        "        aa_residues.append(residue)\n",
        "    return seq, aa_residues"
      ],
      "metadata": {
        "id": "3ORtuAyuujWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHAIN_ID = 'H'\n",
        "\n",
        "# ONLY EMBEDDINGS - NOT CONTACT MAPS\n",
        "def extract_embeddings(model_name, pdb_dir, output_dir, repr_layers=[6]):\n",
        "    \"\"\"   GETS A DIRECTORY OF .pdb FILES, USES AN esm MODEL TO CREATE\n",
        "          EMBEDDINGS FOR A SPECIFIC CHAIN (CHAIN_ID) IN EACH FILE, AND STORES\n",
        "          A .pt FILE WITH AN EMBEDDING VECTOR FOR THAT CHAIN   \"\"\"\n",
        "\n",
        "    model, alphabet = pretrained.load_model_and_alphabet(model_name)\n",
        "    print(\"done download\")\n",
        "    model.eval()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    pdb_dir = pathlib.Path(pdb_dir)\n",
        "    pdb_files = list(pdb_dir.glob('*.pdb'))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for pdb_file in pdb_files:\n",
        "            seq, _ = get_seq_aa(pdb_file, CHAIN_ID)\n",
        "\n",
        "            batch_converter = alphabet.get_batch_converter()\n",
        "            batch_labels, batch_strs, batch_tokens = batch_converter([(str(pdb_file.stem), seq)])\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                batch_tokens = batch_tokens.to(device=\"cuda\", non_blocking=True)\n",
        "\n",
        "            out = model(batch_tokens, repr_layers=repr_layers, return_contacts=False)\n",
        "\n",
        "            representations = {layer: t.to(device=\"cpu\") for layer, t in out[\"representations\"].items()}\n",
        "\n",
        "            for i, label in enumerate(batch_labels):\n",
        "                entry_id = label.split()[0]\n",
        "\n",
        "                filename = output_dir / f\"{entry_id}.pt\"\n",
        "                result = {\"entry_id\": entry_id}\n",
        "\n",
        "                # save amino acid embeddings instead of mean representation\n",
        "                result[\"amino_acid_embeddings\"] = {layer: t[i, 1:-1].clone() for layer, t in representations.items()}\n",
        "                torch.save(result, filename)\n",
        "\n",
        "\n",
        "model_name = 'esm2_t6_8M_UR50D' # esm2_t36_3B_UR50D 'esm1b_t33_650M_UR50S'\n",
        "pdb_dir = '/content/drive/MyDrive/Colab Notebooks/Ex4Data/'\n",
        "output_dir = './hackathon_output/'\n",
        "\n",
        "extract_embeddings(model_name, pdb_dir, pathlib.Path(output_dir))"
      ],
      "metadata": {
        "id": "MUFVwHq4oC7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "READ THE EMBEDDING OF A SPECIFIC PROTEIN. THE PRINTED DATA IS A 2D TENSOR WITH SHAPE: [NUM_AMINO_ACIDS, EMBEDDING_LENGTH].\n"
      ],
      "metadata": {
        "id": "OKkbjMqg8KRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # THIS IS ACTUALLY AN EMBEDDING OF A CHAIN. CHANGE THE EMBEDDING FUNCTION TO GET FOR WHOLE PROTEIN.\n",
        "# filename = '/content/hackathon_output/12E8_1.pt'\n",
        "\n",
        "# data = torch.load(filename)\n",
        "# print(data['amino_acid_embeddings'][33])\n",
        "# #print(len(data['mean_representations'][33]))"
      ],
      "metadata": {
        "id": "hESpl1RG8TwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_input(pt_file):\n",
        "  data = torch.load(pt_file)['amino_acid_embeddings'][6]  # TODO - CHANGE BY NUM OF LAYERS OF esm MODEL\n",
        "  padded_data= torch.zeros((140, 320))  # TODO - CHANGE BY EMBEDDING DIMENTION OF esm MODEL\n",
        "  padded_data[:data.size(0),:] = data\n",
        "  return padded_data.numpy()"
      ],
      "metadata": {
        "id": "1GdQ9pyGvcrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NB_CHAIN_ID = \"H\"\n",
        "NB_MAX_LENGTH = 140\n",
        "AA_DICT = {\"A\": 0, \"C\": 1, \"D\": 2, \"E\": 3, \"F\": 4, \"G\": 5, \"H\": 6, \"I\": 7, \"K\": 8, \"L\": 9, \"M\": 10, \"N\": 11,\n",
        "           \"P\": 12, \"Q\": 13, \"R\": 14, \"S\": 15, \"T\": 16, \"W\": 17, \"Y\": 18, \"V\": 19, \"X\": 20, \"-\": 21}\n",
        "FEATURE_NUM = len(AA_DICT)\n",
        "BACKBONE_ATOMS = [\"N\", \"CA\", \"C\", \"O\", \"CB\"]\n",
        "OUTPUT_SIZE = len(BACKBONE_ATOMS) * 3\n",
        "\n",
        "\n",
        "def generate_label(pdb_file):  # TODO: implement this!\n",
        "    \"\"\"\n",
        "    receives a pdb file and returns its backbone + CB coordinates.\n",
        "    :param pdb_file: path to a pdb file (nanobody, heavy chain has id 'H') already alingned to a reference nanobody.\n",
        "    :return: numpy array of shape (CDR_MAX_LENGTH, OUTPUT_SIZE).\n",
        "    \"\"\"\n",
        "    print(pdb_file)\n",
        "    # Get the sequence of amino acid residues\n",
        "    _, aa_residues = get_seq_aa(pdb_file, NB_CHAIN_ID)\n",
        "\n",
        "    # Create an empty numpy array of shape (140,15) filled with zeroes\n",
        "    coords_matrix = np.zeros((NB_MAX_LENGTH, OUTPUT_SIZE))\n",
        "\n",
        "    # Iterate over the residues and populate the coords_matrix\n",
        "    for i in range(len(aa_residues)):\n",
        "        residue = aa_residues[i]\n",
        "        for j in range(len(BACKBONE_ATOMS)):\n",
        "            atom = BACKBONE_ATOMS[j]\n",
        "            without_gly = atom in residue and residue.get_resname() != \"GLY\"\n",
        "            with_gly = atom in residue and residue.get_resname() == \"GLY\" and j != \"CB\"\n",
        "            if without_gly or with_gly:  # Glycine doesn't have a CB atom\n",
        "                coords_matrix[i, j*3:j*3+3] = residue[atom].get_coord()\n",
        "            else:\n",
        "                coords_matrix[i, j*3:j*3+3] = (0, 0, 0)\n",
        "\n",
        "    return coords_matrix"
      ],
      "metadata": {
        "id": "sNZH7JBjZ-Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATE INPUT FOR NEURAL NET. THIS MATRIX REPRESENTS ALL PROTEINS IN DB. EACH ROW IS AN AMINO ACID EMBEDDING."
      ],
      "metadata": {
        "id": "SNjM_atXBjOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "input_matrix = []\n",
        "labels_matrix = []\n",
        "encodings_data_path = pathlib.Path('./hackathon_output/')  # TODO: change path if needed\n",
        "pdb_data_path = pathlib.Path('/content/drive/MyDrive/Colab Notebooks/Ex4Data/')\n",
        "\n",
        "files_in_pdb = list(pdb_data_path.glob('*'))\n",
        "for protein in files_in_pdb:\n",
        "  protein_no_extention = protein.stem\n",
        "  pdb_path = pdb_data_path / (protein_no_extention + protein.suffix)\n",
        "  embedding_path = encodings_data_path / (protein_no_extention + '.pt')\n",
        "\n",
        "  encoded_protein = generate_input(embedding_path)  # turn torch tensor to numpy array\n",
        "  nb_xyz = generate_label(pdb_path)\n",
        "  input_matrix.append(encoded_protein)\n",
        "  labels_matrix.append(nb_xyz)\n",
        "\n",
        "save_path = Path('/content/drive/MyDrive/Colab Notebooks/final_training_matrices')  # convert string to Path object\n",
        "save_path.mkdir(parents=True, exist_ok=True)\n",
        "np.save(f\"{save_path}/train_input.npy\", np.array(input_matrix))\n",
        "np.save(f\"{save_path}/train_labels.npy\", np.array(labels_matrix))\n",
        "\n",
        "print(f\"Number of samples: {len(input_matrix)}\")\n"
      ],
      "metadata": {
        "id": "pAprLmqaBzxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAKE SURE THAT TRAIN/LABEL MATRICES LOOK OK"
      ],
      "metadata": {
        "id": "0QjrZyvN83bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Path to your .npy file\n",
        "# file_path = '/content/drive/MyDrive/Colab Notebooks/final_training_matrices/train_input.npy'\n",
        "\n",
        "# # Load the data\n",
        "# data = np.load(file_path, allow_pickle=True)\n",
        "\n",
        "# # Print the data\n",
        "# print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cERuzLrIvDWr",
        "outputId": "7c1c32f9-9fe5-4b79-8f5b-bde659694b6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1974, 140, 320)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEST EMBEDDING + CONTCACT"
      ],
      "metadata": {
        "id": "0diwIah2XNs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install biopython\n",
        "!pip install torch\n",
        "!pip install tape_proteins\n",
        "!pip install Bio\n",
        "!pip install import-ipynb\n",
        "!pip install git+https://github.com/facebookresearch/esm.git\n",
        "import pathlib\n",
        "import torch\n",
        "from Bio.PDB import PDBParser, Polypeptide, is_aa\n",
        "from esm import FastaBatchedDataset, pretrained\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n"
      ],
      "metadata": {
        "id": "gosESye-lEeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_embeddings_and_contact_maps(model_name, protain_seq, protain_name):\n",
        "    os.makedirs(utils.path_to_save_emmbending(model_name), exist_ok=True)\n",
        "    repr_layers = [utils.LAYERS_NUMBER[model_name]]\n",
        "    model, alphabet = pretrained.load_model_and_alphabet(model_name)\n",
        "    print(\"done download\")\n",
        "    model.eval()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batch_converter = alphabet.get_batch_converter()\n",
        "        batch_labels, batch_strs, batch_tokens = batch_converter([(protain_name, protain_seq)])\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            batch_tokens = batch_tokens.to(device=\"cuda\", non_blocking=True)\n",
        "\n",
        "        out = model(batch_tokens, repr_layers=repr_layers, return_contacts=True)\n",
        "\n",
        "        representations = {layer: t.to(device=\"cpu\") for layer, t in out[\"representations\"].items()}\n",
        "        contacts = out[\"contacts\"].to(device=\"cpu\")  # New line to extract contact maps\n",
        "\n",
        "        for i, label in enumerate(batch_labels):\n",
        "            entry_id = label.split()[0]\n",
        "\n",
        "            filename = os.path.join(utils.path_to_save_emmbending(model_name), f\"{entry_id}.pt\")\n",
        "            result = {\"entry_id\": entry_id}\n",
        "\n",
        "            # save amino acid embeddings and contact map instead of mean representation\n",
        "            result[\"amino_acid_embeddings\"] = {layer: t[i, 1:-1].clone() for layer, t in representations.items()}\n",
        "            result[\"contact_map\"] = contacts[i]  # Save the contact map\n",
        "\n",
        "            torch.save(result, filename)\n",
        "\n",
        "def generate_input_with_contact_maps(pt_file, model_name):\n",
        "  repr_layers = utils.LAYERS_NUMBER[model_name]\n",
        "  data = torch.load(pt_file)\n",
        "\n",
        "  amino_acid_embeddings = data['amino_acid_embeddings'][repr_layers]\n",
        "  contact_map = data['contact_map']\n",
        "\n",
        "  # Padding both amino acid embeddings and contact maps\n",
        "  padded_embeddings = torch.zeros((140, utils.EMBENDING_DIM[model_name]))\n",
        "  padded_embeddings[:amino_acid_embeddings.size(0),:] = amino_acid_embeddings\n",
        "\n",
        "  padded_contact_map = torch.zeros((140, 140))  # Assuming contact maps are 2D\n",
        "  padded_contact_map[:contact_map.size(0), :contact_map.size(1)] = contact_map\n",
        "\n",
        "  combined_input = np.concatenate((padded_embeddings.numpy(), padded_contact_map.numpy()), axis=1)\n",
        "\n",
        "  return combined_input\n"
      ],
      "metadata": {
        "id": "jqIy27wEk6oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # GET EMBEDDINGS + CONTACT MAPS, AND SAVE THEM TO output_dir AS .pt FILES\n",
        "# def extract_embeddings(model_name, pdb_dir, output_dir, repr_layers=[36]):\n",
        "#     model, alphabet = pretrained.load_model_and_alphabet(model_name)\n",
        "#     model.eval()\n",
        "#     if torch.cuda.is_available():\n",
        "#         model = model.cuda()\n",
        "#     output_dir.mkdir(parents=True, exist_ok=True)\n",
        "#     pdb_dir = pathlib.Path(pdb_dir)\n",
        "#     pdb_files = list(pdb_dir.glob('*.pdb'))\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for pdb_file in pdb_files:\n",
        "#             seq, _ = get_seq_aa(pdb_file, CHAIN_ID)\n",
        "#             batch_converter = alphabet.get_batch_converter()\n",
        "#             batch_labels, batch_strs, batch_tokens = batch_converter([(str(pdb_file.stem), seq)])\n",
        "#             if torch.cuda.is_available():\n",
        "#                 batch_tokens = batch_tokens.to(device=\"cuda\", non_blocking=True)\n",
        "#             out = model(batch_tokens, repr_layers=repr_layers, return_contacts=True)\n",
        "#             representations = {layer: t.to(device=\"cpu\") for layer, t in out[\"representations\"].items()}\n",
        "#             contact_map = out['contacts'].to(device=\"cpu\")\n",
        "#             for i, label in enumerate(batch_labels):\n",
        "#                 entry_id = label.split()[0]\n",
        "#                 filename = output_dir / f\"{entry_id}.pt\"\n",
        "#                 result = {\"entry_id\": entry_id}\n",
        "#                 result[\"amino_acid_embeddings\"] = {layer: t[i, 1:-1].clone() for layer, t in representations.items()}\n",
        "#                 result[\"contact_map\"] = contact_map[i, 1:-1].clone()\n",
        "#                 torch.save(result, filename)\n",
        "\n",
        "# model_name = 'esm2_t36_3B_UR50D' # esm2_t36_3B_UR50D 'esm1b_t33_650M_UR50S'\n",
        "# pdb_dir = '/content/drive/MyDrive/Colab Notebooks/Ex4Data/'\n",
        "# output_dir = '/content/drive/MyDrive/Colab Notebooks/embeddings_for_contacts/'\n",
        "# extract_embeddings(model_name, pdb_dir, pathlib.Path(output_dir))"
      ],
      "metadata": {
        "id": "3TNfzRV6GlU_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}